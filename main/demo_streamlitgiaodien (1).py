import streamlit as st
import pandas as pd
import joblib
import re
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import wordninja
import unicodedata
import sys
import numpy as np
import pickle
import base64
from PIL import Image
import os, base64, mimetypes
import streamlit as st

# T·∫£i d·ªØ li·ªáu NLTK c·∫ßn thi·∫øt
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn NLP
stop_w = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
punctuation = dict.fromkeys((i for i in range(sys.maxunicode)
                            if unicodedata.category(chr(i)).startswith("P")), " ")

# ƒê·ªãnh nghƒ©a c√°c h√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
def remove_url(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = re.sub(r"http?://\S+\#|www\.\S+\#", " ", text)
    return re.sub(r"#(URL|url)_.*#", " ", text)

def remove_component_html(text: str):
    if not isinstance(text, str):
        return ""
    clean_text = re.sub(r"&[a-zA-Z0-9#]+;", "", text)
    soup = BeautifulSoup(clean_text, features="html.parser")
    return soup.text

def remove_non_english_letters(text):
    if not isinstance(text, str):
        return ""
    return re.sub(r'[^\W\d_]', lambda m: '' if not re.match(r'[a-zA-Z]', m.group()) else m.group(), text)

def nomalize_space(text):
    if not isinstance(text, str):
        return ""
    return re.sub(r'\s+', ' ', text).strip()

def split_and_segment_words(text):
    if not isinstance(text, str):
        return ""
    # T√°ch camel case (ch·ªØ hoa)
    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)
    
    # T√°ch t·ª´ d√≠nh b·∫±ng wordninja
    tokens = text.split()
    segmented = []
    for token in tokens:
        segmented.extend(wordninja.split(token))
    
    return ' '.join(segmented)

def nlp_process(text):
    if not isinstance(text, str):
        return []
    
    text = text.lower()
    text = text.replace(".", " ")
    text = remove_url(text)
    text = remove_component_html(text)
    text = remove_non_english_letters(text)
    text = split_and_segment_words(text)
    
    text = text.translate(punctuation)
    text = nomalize_space(text)

    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_w]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

# T·∫£i m√¥ h√¨nh
@st.cache_resource
def load_model():
    try:
        model = joblib.load('C:\\Users\\DELL 15\\Documents\\Zalo Received Files\\fraud_detection_model_smote.pkl')
        st.sidebar.success("‚úÖ Model loaded successfully!")
        return model
    except Exception as e:
        st.sidebar.error(f"‚ùå Model loading failed: {str(e)}")
        return None

model = load_model() 
WINDOWS_LOGO_PATH = r"C:\Users\DELL 15\Pictures\da-hoc-khoa-hoc-hue-3png-1.webp"  # <-- ch·ªânh ƒë√∫ng file

def _logo_src(path_asset="assets/icon/logo.png", path_win=WINDOWS_LOGO_PATH):
    chosen = path_win if (path_win and os.path.exists(path_win)) else path_asset
    if not os.path.exists(chosen):
        return None
    mime = mimetypes.guess_type(chosen)[0] or "image/png"
    with open(chosen, "rb") as f:
        b64 = base64.b64encode(f.read()).decode()
    return f"data:{mime};base64,{b64}"

def show_logo(top_px=100, right_px=24, width=70):
    src = _logo_src()
    if not src:
        return
    html = """
    <style>
      .logo-fixed { position: fixed; top: %dpx; right: %dpx; z-index: 1000; }
    </style>
    <div class="logo-fixed">
      <img src="%s" width="%d"/>
    </div>
    """ % (top_px, right_px, src, width)
    st.markdown(html, unsafe_allow_html=True)

show_logo()
# ==== END Floating Logo ====

with st.expander("üìò H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng ·ª©ng d·ª•ng"):
    st.markdown("""
    - Nh·∫≠p **Ti√™u ƒë·ªÅ** v√† **M√¥ t·∫£ c√¥ng vi·ªác** l√† b·∫Øt bu·ªôc.
    - C√°c tr∆∞·ªùng kh√°c gi√∫p m√¥ h√¨nh ho·∫°t ƒë·ªông ch√≠nh x√°c h∆°n.
    - ·ª®ng d·ª•ng s·ª≠ d·ª•ng m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ d·ª± ƒëo√°n m·ª©c ƒë·ªô ƒë√°ng tin c·∫≠y c·ªßa tin tuy·ªÉn d·ª•ng.
    """)
# Giao di·ªán Streamlit
st.title("üïµÔ∏è Fake Job Posting Detector")
st.markdown("Nh·∫≠p th√¥ng tin c√¥ng vi·ªác ƒë·ªÉ ki·ªÉm tra kh·∫£ nƒÉng l√† tin tuy·ªÉn d·ª•ng gi·∫£")

with st.form("job_form"):
    st.header("Th√¥ng tin c√¥ng vi·ªác")
    
    col1, col2 = st.columns(2)
    # C·ªôt 1
    title = col1.text_input("Ti√™u ƒë·ªÅ*", help="Ti√™u ƒë·ªÅ c√¥ng vi·ªác")
    location = col1.text_input("ƒê·ªãa ƒëi·ªÉm", help="ƒê·ªãa ƒëi·ªÉm l√†m vi·ªác")
    department = col1.text_input("Ph√≤ng ban", help="Ph√≤ng ban tuy·ªÉn d·ª•ng")
    salary_range = col1.text_input("M·ª©c l∆∞∆°ng", help="V√≠ d·ª•: $50,000 - $70,000")
    
    # C·ªôt 2
    telecommuting = col2.checkbox("L√†m vi·ªác t·ª´ xa", help="C√≥ th·ªÉ l√†m vi·ªác t·ª´ xa")
    has_company_logo = col2.checkbox("C√≥ logo c√¥ng ty", help="Tin c√≥ logo c√¥ng ty")
    has_questions = col2.checkbox("C√≥ c√¢u h·ªèi ·ª©ng tuy·ªÉn", help="Y√™u c·∫ßu tr·∫£ l·ªùi c√¢u h·ªèi khi ·ª©ng tuy·ªÉn")
    
    # Tr∆∞·ªùng vƒÉn b·∫£n d√†i
    company_profile = st.text_area("Th√¥ng tin c√¥ng ty", height=100)
    description = st.text_area("M√¥ t·∫£ c√¥ng vi·ªác*", height=150)
    requirements = st.text_area("Y√™u c·∫ßu c√¥ng vi·ªác", height=150)
    benefits = st.text_area("Ph√∫c l·ª£i", height=100)
    
    # Tr∆∞·ªùng ph√¢n lo·∫°i
    exp_options = ["Not Applicable", "Associate", "Director", "Entry level", 
                  "Executive", "Internship", "Mid-Senior level"]
    
    edu_options = ["Unspecified", "Bachelor's Degree", "Master's Degree", 
                  "High School or equivalent", "Some College Coursework Completed",
                  "Vocational", "Certification", "Associate Degree", "Professional"]
    
    type_options = ["Other","Full-time", "Part-time", "Contract", "Temporary", "Internship"]
    
    col3, col4, col5 = st.columns(3)
    employment_type = col3.selectbox("Lo·∫°i h√¨nh c√¥ng vi·ªác", options=type_options)
    required_experience = col4.selectbox("Y√™u c·∫ßu kinh nghi·ªám", options=exp_options)
    required_education = col5.selectbox("Y√™u c·∫ßu h·ªçc v·∫•n", options=edu_options)
    
    col6, col7 = st.columns(2)
    industry = col6.text_input("Ng√†nh ngh·ªÅ", help="V√≠ d·ª•: C√¥ng ngh·ªá th√¥ng tin")
    function = col7.text_input("Ch·ª©c nƒÉng c√¥ng vi·ªác", help="Lo·∫°i c√¥ng vi·ªác")
    
    # N·ªôp form
    submitted = st.form_submit_button("Ki·ªÉm tra c√¥ng vi·ªác")
    
    if submitted:
        # Ki·ªÉm tra tr∆∞·ªùng b·∫Øt bu·ªôc
        if not title or not description:
            st.error("Vui l√≤ng nh·∫≠p Ti√™u ƒë·ªÅ v√† M√¥ t·∫£ c√¥ng vi·ªác")
        else:
            # T·∫°o dictionary d·ªØ li·ªáu
            job_data = {
                "title": title,
                "location": location,
                "department": department,
                "salary_range": salary_range,
                "company_profile": company_profile,
                "description": description,
                "requirements": requirements,
                "benefits": benefits,
                "telecommuting": int(telecommuting),
                "has_company_logo": int(has_company_logo),
                "has_questions": int(has_questions),
                "employment_type": employment_type,
                "required_experience": required_experience,
                "required_education": required_education,
                "industry": industry,
                "function": function
            }
            
            # T·∫°o c·ªôt missing
            job_data["missing_employment_type"] = int(not employment_type)
            job_data["missing_benefits"] = int(not benefits)
            job_data["missing_description"] = int(not description)
            job_data["missing_company_profile"] = int(not company_profile)
            job_data["missing_location"] = int(not location)
            job_data["missing_department"] = int(not department)
            
            # T·∫°o c·ªôt text
            text_fields = [title, location, department, description, 
                          company_profile, benefits, requirements, salary_range]
            job_data["text"] = " ".join(filter(None, text_fields))
            
            # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
            job_data["text"] = job_data["text"].lower()
            job_data["text"] = remove_url(job_data["text"])
            job_data["text"] = remove_component_html(job_data["text"])
            job_data["text"] = remove_non_english_letters(job_data["text"])
            job_data["text"] = split_and_segment_words(job_data["text"])
            job_data["text"] = job_data["text"].translate(punctuation)
            job_data["text"] = nomalize_space(job_data["text"])
            
            # T·∫°o DataFrame
            df = pd.DataFrame([job_data])
            
            # S·∫Øp x·∫øp c·ªôt theo ƒë√∫ng th·ª© t·ª± m√¥ h√¨nh y√™u c·∫ßu
            columns_order = [
                'telecommuting', 'has_company_logo', 'has_questions',
                'missing_employment_type', 'missing_benefits', 'missing_description',
                'missing_company_profile', 'missing_location', 'missing_department',
                'text', 'employment_type', 'required_experience',
                'required_education', 'industry', 'function'
            ]
            df = df[columns_order]
            
            # D·ª± ƒëo√°n
            try:
                prediction = model.predict(df)
                prediction_proba = model.predict_proba(df)
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                st.subheader("K·∫øt qu·∫£ ki·ªÉm tra")
                if prediction[0] == 1:
                    st.error("‚ö†Ô∏è C·∫¢NH B√ÅO: C√¥ng vi·ªác c√≥ kh·∫£ nƒÉng l√† TIN TUY·ªÇN D·ª§NG GI·∫¢")
                else:
                    st.success("‚úÖ C√¥ng vi·ªác c√≥ v·∫ª l√† TIN TUY·ªÇN D·ª§NG TH·∫¨T")
                
                # Hi·ªÉn th·ªã x√°c su·∫•t
                proba = prediction_proba[0][0] * 100
                st.metric(label="ƒê·ªô tin c·∫≠y d·ª± ƒëo√°n", 
                          value=f"{proba:.1f}%", 
                          help="T·ª∑ l·ªá ph·∫ßn trƒÉm kh·∫£ nƒÉng l√† c√¥ng vi·ªác th·∫≠t")
                #proba = prediction_proba[0][1] * 100
                    #st.metric(label="ƒê·ªô tin c·∫≠y d·ª± ƒëo√°n", 
                                #value=f"{proba:.1f}%", 
                               # help="T·ª∑ l·ªá ph·∫ßn trƒÉm kh·∫£ nƒÉng l√† c√¥ng vi·ªác gi·∫£")
                
                # Gi·∫£i th√≠ch k·∫øt qu·∫£
                st.subheader("Ph√¢n t√≠ch k·∫øt qu·∫£")
                if prediction[0] == 1:
                    st.write("M√¥ h√¨nh c·ªßa ch√∫ng t√¥i ƒë√£ ph√°t hi·ªán c√°c d·∫•u hi·ªáu ƒë√°ng ng·ªù trong tin tuy·ªÉn d·ª•ng n√†y. "
                             "D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë y·∫øu t·ªë th∆∞·ªùng g·∫∑p trong tin tuy·ªÉn d·ª•ng gi·∫£:")
                    st.markdown("- Thi·∫øu th√¥ng tin chi ti·∫øt v·ªÅ c√¥ng ty ho·∫∑c c√¥ng vi·ªác")
                    st.markdown("- M√¥ t·∫£ c√¥ng vi·ªác m∆° h·ªì, kh√¥ng r√µ r√†ng")
                    st.markdown("- Y√™u c·∫ßu qu√° ƒë∆°n gi·∫£n so v·ªõi m·ª©c l∆∞∆°ng h·∫•p d·∫´n")
                    st.markdown("- Th√¥ng tin li√™n h·ªá kh√¥ng ƒë√°ng tin c·∫≠y")
                else:
                    st.write("Tin tuy·ªÉn d·ª•ng n√†y c√≥ v·∫ª h·ª£p l·ªá v·ªõi c√°c th√¥ng tin chi ti·∫øt v√† minh b·∫°ch. "
                             "Tuy nhi√™n, lu√¥n c·∫£nh gi√°c v·ªõi:")
                    st.markdown("- Y√™u c·∫ßu thanh to√°n tr∆∞·ªõc khi nh·∫≠n vi·ªác")
                    st.markdown("- C√¥ng ty kh√¥ng c√≥ th√¥ng tin r√µ r√†ng tr√™n m·∫°ng")
                    st.markdown("- Quy tr√¨nh tuy·ªÉn d·ª•ng qu√° ƒë∆°n gi·∫£n")
                
                # Hi·ªÉn th·ªã d·ªØ li·ªáu ƒë√£ nh·∫≠p
                st.subheader("D·ªØ li·ªáu ƒë√£ nh·∫≠p")
                st.dataframe(df.iloc[:, :10])  # Hi·ªÉn th·ªã 10 c·ªôt ƒë·∫ßu
                
            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra khi d·ª± ƒëo√°n: {str(e)}")

# Th√¥ng tin th√™m
st.markdown("---")
st.subheader("L∆∞u √Ω quan tr·ªçng")
st.markdown("""
- M√¥ h√¨nh n√†y ch·ªâ cung c·∫•p ƒë√°nh gi√° d·ª±a tr√™n d·ªØ li·ªáu nh·∫≠p v√†o
- K·∫øt qu·∫£ d·ª± ƒëo√°n kh√¥ng ƒë·∫£m b·∫£o 100% ch√≠nh x√°c
- Lu√¥n ki·ªÉm tra k·ªπ th√¥ng tin c√¥ng ty v√† quy tr√¨nh tuy·ªÉn d·ª•ng
- Kh√¥ng cung c·∫•p th√¥ng tin c√° nh√¢n nh·∫°y c·∫£m tr∆∞·ªõc khi x√°c minh
- C√°c tr∆∞·ªùng c√≥ d·∫•u * l√† b·∫Øt bu·ªôc
""")

st.caption("·ª®ng d·ª•ng ph√°t hi·ªán tin tuy·ªÉn d·ª•ng gi·∫£ | Phi√™n b·∫£n 1.0 | S·ª≠ d·ª•ng m√¥ h√¨nh Machine Learning")
## ƒê√≥ng g√≥i file pkl c·ªßa m√¥ h√¨nh 
## vd pipeline.fit(X_train, y_train)

# ƒê√°nh gi√° m√¥ h√¨nh
#y_pred = pipeline.predict(X_test)

#print("\nModel Evaluation:")
#print("Accuracy:", accuracy_score(y_test, y_pred))
#print(classification_report(y_test, y_pred))

# L∆∞u m√¥ h√¨nh th√†nh file .pkl
#dump(pipeline, 'fraud_detection_model.pkl')
#print("\nModel has been saved as 'fraud_detection_model.pkl'")
# ƒê·ªÉ load l·∫°i m√¥ h√¨nh sau n√†y, s·ª≠ d·ª•ng:
# pipeline = load('fraud_detection_model.pkl')

## Sau ƒë√≥ doc file tai mo hinh vao de train 
# T·∫£i m√¥ h√¨nh
#@st.cache_resource
#def load_model():
 #   try:
  #      model = joblib.load('C:\\Users\\DELL 15\\Documents\\Zalo Received Files\\fraud_detection_model.pkl')
   #     st.sidebar.success(" Model loaded successfully!")
    #    return model
    #except Exception as e:
     #   st.sidebar.error(f" Model loading failed: {str(e)}")
      #  return None

#model = load_model()

#v·∫•n ƒë·ªÅ g·∫∑p ph·∫£i l√† ph·∫£i ch·∫°y tr√™n cmd m·ªõi ra d·ª± √°n ƒë∆∞·ª£c c·∫ßn bi·∫øn n√≥ th√†nh link ƒë·ªÉ demo

##  cd "C:\Users\DELL 15\Downloads"
## python -m streamlit run demo_streamlitgiaodien.py (code ƒë·ªÉ ch·∫°y)